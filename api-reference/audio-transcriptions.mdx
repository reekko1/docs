---
title: 'Voice to Text'
description: 'Voice-to-text for simplified clinical log creation'
---
The Audio Transcription endpoint converts voice recordings into text to simplify clinical log writing. Doctors can record their observations verbally and get accurate transcriptions that serve as starting points for log entries - eliminating the need to type from scratch while maintaining full control over the final content.

<Info>
**For newcomers**: Think of this as our "voice-to-draft" converter. Instead of typing "Patient's wound looks better today, less drainage," doctors can just speak it and get clean text ready to review and post. It's about making documentation faster, not automated.

**For senior devs**: Groq Whisper-large-v3 integration for speech-to-text, supports all standard audio formats, returns clean transcription text that feeds into the log creation workflow. No direct posting - always requires human review and confirmation.
</Info>

## Overview

### Key Features
- **High-accuracy transcription** - Powered by Groq's Whisper-large-v3 model
- **Multiple audio formats** - Supports MP3, WAV, M4A, FLAC, and more
- **Medical-ready output** - Clean, readable text suitable for clinical documentation
- **Review-first workflow** - Transcriptions are drafts, not final posts
- **Fast processing** - Synchronous transcription for immediate feedback

### Clinical Workflow
1. **Record** - Doctor speaks clinical observations
2. **Transcribe** - Audio converted to text via this endpoint
3. **Review** - Doctor reviews and edits transcription
4. **Post** - Final text submitted as log entry

### Why We Built It This Way
- **Human oversight** - Medical documentation requires physician review
- **Speed without automation** - Faster than typing, safer than auto-posting
- **Flexible editing** - Doctors can modify transcriptions before submission
- **Clean output** - No timestamps or speaker labels, just readable text

## Authentication

```bash
X-API-Key: your-api-key
```

Audio transcription requires API key authentication.

## Endpoint

### Transcribe Audio

<CodeGroup>

```bash cURL
curl -X POST https://api.narra.com/api/v1/audio-transcriptions \
  -H "X-API-Key: your-api-key" \
  -F "file=@clinical_note.mp3"
```

```python Python
import requests

# Transcribe audio recording
with open('clinical_note.m4a', 'rb') as audio_file:
    files = {'file': ('clinical_note.m4a', audio_file, 'audio/x-m4a')}
    
    response = requests.post(
        "https://api.narra.com/api/v1/audio-transcriptions",
        headers={"X-API-Key": "your-api-key"},
        files=files
    )
    
    transcription = response.json()
    print(transcription['transcription_text'])
```

```javascript JavaScript
// Transcribe audio from mobile recording
const transcribeAudio = async (audioUri) => {
  const formData = new FormData();
  formData.append('file', {
    uri: audioUri,
    type: 'audio/m4a',
    name: 'recording.m4a'
  });
  
  const response = await fetch('/api/v1/audio-transcriptions', {
    method: 'POST',
    headers: {
      'X-API-Key': apiKey,
    },
    body: formData
  });
  
  const result = await response.json();
  return result.transcription_text;
};
```

</CodeGroup>

**POST** `/audio-transcriptions`

Transcribes an audio recording into clean, readable text suitable for clinical documentation.

### Request Body (multipart/form-data)

<ParamField body="file" type="file" required>
  Audio file to transcribe. Supported formats:
  - **MP3** (.mp3) - Most common format
  - **M4A** (.m4a) - iPhone/iOS recordings
  - **WAV** (.wav) - High quality uncompressed
  - **FLAC** (.flac) - Lossless compression
  - **OGG** (.ogg) - Open source format
  - **WebM** (.webm) - Web audio format
  - **MP4** (.mp4) - Video container with audio
  - **MPEG/MPGA** - MPEG audio formats
</ParamField>

### Response

```json
{
  "transcription_text": "Patient reports decreased pain in left knee, now 3 out of 10. Ambulating without assistance. Wound healing well with no signs of infection.",
  "model_used": "whisper-large-v3-turbo"
}
```

<ResponseField name="transcription_text" type="string">
  The transcribed text, ready for review and editing before posting as a log entry
</ResponseField>

<ResponseField name="model_used" type="string">
  The AI model used for transcription (always "whisper-large-v3-turbo")
</ResponseField>

## Transcription Examples

### Clinical Assessment Recording
**Audio**: *"Patient's vital signs are stable today. Blood pressure one twenty over eighty. Heart rate seventy-two. Temperature ninety-eight point six. Patient reports feeling much better and pain is now down to a two out of ten."*

**Transcription**:
```
Patient's vital signs are stable today. Blood pressure 120 over 80. Heart rate 72. Temperature 98.6. Patient reports feeling much better and pain is now down to a 2 out of 10.
```

### Wound Assessment Recording
**Audio**: *"Examining the surgical site on the right leg. The wound measures approximately three centimeters. There's good granulation tissue present. No signs of infection. Drainage has decreased significantly since yesterday."*

**Transcription**:
```
Examining the surgical site on the right leg. The wound measures approximately 3 centimeters. There's good granulation tissue present. No signs of infection. Drainage has decreased significantly since yesterday.
```

### Treatment Plan Recording
**Audio**: *"Plan for today includes continuing the current antibiotic regimen. Patient will start physical therapy tomorrow. We'll reassess the wound in forty-eight hours. Family education provided regarding dressing changes."*

**Transcription**:
```
Plan for today includes continuing the current antibiotic regimen. Patient will start physical therapy tomorrow. We'll reassess the wound in 48 hours. Family education provided regarding dressing changes.
```

## Audio Quality Tips

### Best Practices for Recording
<Tip>
**DO:**
- Speak clearly and at normal pace
- Record in quiet environment
- Hold device close to mouth (6-12 inches)
- Pause between different topics
- Spell out unusual medication names

**DON'T:**
- Record in noisy environments
- Speak too quickly or mumble
- Use abbreviations that sound unclear
- Record with background conversations
</Tip>

### Optimal Recording Conditions
- **Quiet environment** - Minimal background noise
- **Clear speech** - Normal speaking pace and volume
- **Good microphone** - Built-in phone microphones work well
- **Short segments** - 1-3 minutes per recording for best accuracy

## Error Handling

<ResponseField name="400 Bad Request">
  Invalid file format or missing file
  ```json
  {
    "detail": "Invalid file type. Supported types: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm"
  }
  ```
</ResponseField>

<ResponseField name="500 Internal Server Error">
  Missing Groq API key or transcription service error
  ```json
  {
    "detail": "GROQ_API_KEY not configured"
  }
  ```
</ResponseField>

<ResponseField name="503 Service Unavailable">
  Groq API unavailable or rate limited
</ResponseField>

## Integration Examples

### Mobile App Voice Recording Workflow

```typescript
// React Native audio recording for log creation
import AudioRecorderPlayer from 'react-native-audio-recorder-player';

const VoiceToLogWorkflow = () => {
  const [isRecording, setIsRecording] = useState(false);
  const [transcriptionText, setTranscriptionText] = useState('');
  const [isEditing, setIsEditing] = useState(false);
  
  const audioRecorderPlayer = new AudioRecorderPlayer();
  
  const startRecording = async () => {
    setIsRecording(true);
    await audioRecorderPlayer.startRecorder();
  };
  
  const stopRecording = async () => {
    const audioPath = await audioRecorderPlayer.stopRecorder();
    setIsRecording(false);
    
    // Transcribe the recording
    await transcribeRecording(audioPath);
  };
  
  const transcribeRecording = async (audioPath: string) => {
    const formData = new FormData();
    formData.append('file', {
      uri: audioPath,
      type: 'audio/m4a',
      name: 'clinical_note.m4a'
    });
    
    try {
      const response = await fetch('/audio-transcriptions', {
        method: 'POST',
        headers: { 'X-API-Key': apiKey },
        body: formData
      });
      
      const result = await response.json();
      setTranscriptionText(result.transcription_text);
      setIsEditing(true); // Let doctor review/edit
      
    } catch (error) {
      showError('Transcription failed. Please try again.');
    }
  };
  
  const submitLog = async () => {
    // User reviewed and approved the transcription
    const logData = {
      log: transcriptionText,
      patient_id: patientId,
      user_id: userId,
      workspace_id: workspaceId
    };
    
    // Post to log stream
    const response = await fetch('/log/stream', {
      method: 'POST',
      headers: { 
        'X-API-Key': apiKey,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(logData)
    });
    
    // Handle streaming response...
  };
  
  return (
    <View>
      {!isEditing ? (
        <RecordingInterface 
          isRecording={isRecording}
          onStart={startRecording}
          onStop={stopRecording}
        />
      ) : (
        <TranscriptionReview
          text={transcriptionText}
          onEdit={setTranscriptionText}
          onSubmit={submitLog}
          onCancel={() => setIsEditing(false)}
        />
      )}
    </View>
  );
};
```

### Web App Dictation Interface

```typescript
// Web-based voice recording with review flow
class VoiceDictation {
  private mediaRecorder: MediaRecorder | null = null;
  private chunks: Blob[] = [];
  
  async startDictation() {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    this.mediaRecorder = new MediaRecorder(stream);
    this.chunks = [];
    
    this.mediaRecorder.ondataavailable = (event) => {
      this.chunks.push(event.data);
    };
    
    this.mediaRecorder.onstop = async () => {
      const audioBlob = new Blob(this.chunks, { type: 'audio/webm' });
      await this.transcribeAudio(audioBlob);
    };
    
    this.mediaRecorder.start();
  }
  
  stopDictation() {
    if (this.mediaRecorder && this.mediaRecorder.state === 'recording') {
      this.mediaRecorder.stop();
    }
  }
  
  private async transcribeAudio(audioBlob: Blob) {
    const formData = new FormData();
    formData.append('file', audioBlob, 'recording.webm');
    
    const response = await fetch('/audio-transcriptions', {
      method: 'POST',
      headers: { 'X-API-Key': apiKey },
      body: formData
    });
    
    const { transcription_text } = await response.json();
    
    // Show transcription for review
    this.showTranscriptionReview(transcription_text);
  }
  
  private showTranscriptionReview(text: string) {
    // Display editable transcription
    const reviewModal = document.createElement('div');
    reviewModal.innerHTML = `
      <div class="transcription-review">
        <h3>Review Your Transcription</h3>
        <textarea id="transcription-text">${text}</textarea>
        <button onclick="this.approveTranscription()">Use This Text</button>
        <button onclick="this.cancelTranscription()">Try Again</button>
      </div>
    `;
    document.body.appendChild(reviewModal);
  }
}
```

### Batch Audio Processing

```python
# Process multiple audio files for bulk transcription
import os
import requests
from concurrent.futures import ThreadPoolExecutor

def transcribe_audio_file(file_path: str) -> dict:
    """Transcribe a single audio file"""
    with open(file_path, 'rb') as f:
        files = {'file': (os.path.basename(file_path), f, 'audio/m4a')}
        
        response = requests.post(
            "https://api.narra.com/api/v1/audio-transcriptions",
            headers={"X-API-Key": api_key},
            files=files
        )
        
        return {
            'file': file_path,
            'transcription': response.json()['transcription_text'],
            'success': response.status_code == 200
        }

def batch_transcribe_recordings(audio_directory: str):
    """Transcribe all audio files in a directory"""
    audio_files = [
        os.path.join(audio_directory, f) 
        for f in os.listdir(audio_directory) 
        if f.endswith(('.mp3', '.m4a', '.wav'))
    ]
    
    # Process up to 5 files concurrently
    with ThreadPoolExecutor(max_workers=5) as executor:
        results = list(executor.map(transcribe_audio_file, audio_files))
    
    return results

# Usage
results = batch_transcribe_recordings('./clinical_recordings/')
for result in results:
    if result['success']:
        print(f"File: {result['file']}")
        print(f"Transcription: {result['transcription']}")
        print("---")
```

## Performance Tips

1. **Keep recordings short** - 1-3 minutes for best accuracy and speed
2. **Good audio quality** - Clear speech improves transcription accuracy
3. **Review and edit** - Always review transcriptions before posting
4. **Batch when possible** - Process multiple recordings if needed

## Common Use Cases

### Quick Status Updates
Record brief patient status updates during rounds:
*"Room 302, patient stable, pain controlled, ready for discharge planning."*

### Procedure Documentation
Document procedures as they happen:
*"Performed wound care on left lower extremity. Cleaned with saline, applied antimicrobial dressing. Patient tolerated well."*

### Family Communication Notes
Record important family discussions:
*"Spoke with patient's daughter about treatment plan. She understands the risks and agrees to proceed with surgery tomorrow."*

## Debugging

<Tip>
Check these for transcription issues:
- **Audio quality**: Ensure clear, noise-free recording
- **File format**: Verify supported audio format
- **File size**: Large files may timeout or fail
- **Network**: Check connectivity for API calls
- **API keys**: Verify Groq API key is configured
</Tip>

## Related Endpoints

- [Clinical Log Processing](/api-reference/log-stream) - Post reviewed transcriptions as clinical logs
- [Photo to Text](/api-reference/vision-logs) - Similar workflow for image-to-text conversion
- [Media Uploads](/api-reference/uploads) - Upload and store audio files permanently